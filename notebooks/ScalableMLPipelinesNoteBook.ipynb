{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq torch==2.0.1 --progress-bar off\n",
    "!pip install -qqq transformers==4.31 --progress-bar off\n",
    "!pip install -qqq langchain==0.0.266 --progress-bar off\n",
    "!pip install -qqq pypdf==3.15.0 --progress-bar off\n",
    "!pip install -qqq xformers==0.0.20 --progress-bar off\n",
    "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off\n",
    "!pip install -qqq InstructorEmbedding==1.0.1 --progress-bar off\n",
    "!pip install -q kedro --progress-bar off\n",
    "!wget -qqq https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.4.1/auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl\n",
    "!pip install -qqq auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl --progress-bar off\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b165f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/RCostaBooks/ScalableMLPipelines/main/code/config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d403a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kedro new -c config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e1a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -P ./chatbot/data/01_raw https://raw.githubusercontent.com/RCostaBooks/ScalableMLPipelines/main/pdfs/2022-TSLA-F10K.pdf\n",
    "!wget -q -P ./chatbot/data/01_raw https://raw.githubusercontent.com/RCostaBooks/ScalableMLPipelines/main/pdfs/2022-NVDA-F10K.pdf\n",
    "!wget -q -P ./chatbot/data/01_raw https://raw.githubusercontent.com/RCostaBooks/ScalableMLPipelines/main/pdfs/2022-AMD-F10K.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd chatbot\n",
    "!kedro pipeline create ingestDocuments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12c525",
   "metadata": {},
   "source": [
    "## Code: Pipeline from ingestDocuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kedro.pipeline import Pipeline, node, pipeline\n",
    "from .nodes import loadDocuments, textSplitting, createEmbeddings\n",
    "\n",
    "def create_pipeline(**kwargs) -> Pipeline:\n",
    "    return pipeline(\n",
    "        [\n",
    "            node(\n",
    "                func=loadDocuments,\n",
    "                inputs=None,\n",
    "                outputs=\"docs\",\n",
    "                name=\"loadDocuments_node\",\n",
    "            ),\n",
    "            node(\n",
    "                func=textSplitting,\n",
    "                inputs=\"docs\",\n",
    "                outputs=\"texts\",\n",
    "                name=\"textSplitting_node\",\n",
    "            ),\n",
    "            node(\n",
    "                func=createEmbeddings,\n",
    "                inputs=\"texts\",\n",
    "                outputs=None,\n",
    "                name=\"createEmbeddings_node\",\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can download it...\n",
    "!wget -q -P ./src/chatbot/pipelines/ingestDocuments https://raw.githubusercontent.com/RCostaBooks/ScalableMLPipelines/main/code/pipeline.py -O ./src/chatbot/pipelines/ingestDocuments/pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb8821",
   "metadata": {},
   "source": [
    "## Code: Nodes from Pipeline ingestDocuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959d9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import SKLearnVectorStore\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def loadDocuments() -> PyPDFDirectoryLoader:\n",
    "  log.info(\"Starting PDFLoader Node.\")\n",
    "\n",
    "  loader = PyPDFDirectoryLoader(\"./data/01_raw\")\n",
    "  docs = loader.load()\n",
    "  log.info(f\"Total pages loaded: {len(docs)}\")\n",
    "  return docs\n",
    "\n",
    "def textSplitting(docs: PyPDFDirectoryLoader) -> List:\n",
    "  log.info('Splitting Text into Chunks...')\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "  texts = text_splitter.split_documents(docs)\n",
    "  log.info(f\"Total Chunks created: {len(texts)}\")\n",
    "  return texts\n",
    "\n",
    "def createEmbeddings(texts: List):\n",
    "  log.info(f'Loading Embedding Model on {DEVICE}...')\n",
    "\n",
    "  embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-large\",\n",
    "    model_kwargs={\"device\": DEVICE}\n",
    "  )\n",
    "\n",
    "  log.info('Saving Persistent Vector Database')\n",
    "  db = SKLearnVectorStore.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings,\n",
    "    persist_path=\"./data/05_model_input/vectordb.parquet\",\n",
    "    serializer=\"parquet\"\n",
    "    )\n",
    "  db.persist()\n",
    "\n",
    "  log.info('Ingestion process completed for all pdf files.')\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869fd850",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -P ./src/chatbot/pipelines/ingestDocuments https://raw.githubusercontent.com/RCostaBooks/ScalableMLPipelines/main/code/nodes.py -O ./src/chatbot/pipelines/ingestDocuments/nodes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...finally\n",
    "!kedro run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f7e168",
   "metadata": {},
   "source": [
    "## API Server Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flask-ngrok\n",
    "!pip install pyngrok\n",
    "!ngrok authtoken 'YOUR NGROK API KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_ngrok import run_with_ngrok\n",
    "from langchain.vectorstores import SKLearnVectorStore\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.output_parsers import RegexParser\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline\n",
    "from auto_gptq import AutoGPTQForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-large\",\n",
    "    model_kwargs={\"device\": DEVICE}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f704223",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path= \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True,\n",
    "    inject_fused_attention=False,\n",
    "    device=DEVICE,\n",
    "    quantize_config=None,\n",
    ")\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    streamer=streamer,\n",
    "    )\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store2 = SKLearnVectorStore(\n",
    "    embedding=embeddings, persist_path=\"./chatbot/data/05_model_input/vectordb.parquet\", serializer=\"parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "This should be in the following format:\n",
    "\n",
    "Question: [question here]\n",
    "Helpful Answer: [answer here]\n",
    "Score: [score between 0 and 100]\n",
    "\n",
    "Begin!\n",
    "\n",
    "Context:\n",
    "---------\n",
    "{context}\n",
    "---------\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "output_parser = RegexParser(\n",
    "    regex=r\"(.*?)\\nScore: (.*)\",\n",
    "    output_keys=[\"answer\", \"score\"],\n",
    ")\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    output_parser=output_parser\n",
    ")\n",
    "\n",
    "chain = load_qa_chain(llm=llm, chain_type=\"map_rerank\",\n",
    "                      return_intermediate_steps=True, prompt=PROMPT)\n",
    "\n",
    "\n",
    "def getanswer(query):\n",
    "    print(f'Function getanswer...{query}')\n",
    "    relevant_chunks = vector_store2.similarity_search_with_score(query, k=2)\n",
    "    print(f'relevant: {relevant_chunks}')\n",
    "    chunk_docs = []\n",
    "    for chunk in relevant_chunks:\n",
    "        chunk_docs.append(chunk[0])\n",
    "    results = chain({\"input_documents\": chunk_docs, \"question\": query})\n",
    "    text_reference = \"\"\n",
    "    for i in range(len(results[\"input_documents\"])):\n",
    "        text_reference += results[\"input_documents\"][i].page_content\n",
    "    output = {\"Answer\": results[\"output_text\"], \"Reference\": text_reference}\n",
    "    return output\n",
    "\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)\n",
    "\n",
    "@app.route('/', methods=[\"POST\"])\n",
    "def processclaim():\n",
    "    print('Got Question...')\n",
    "    try:\n",
    "        input_json = request.get_json(force=True)\n",
    "        print(f'Question: {input_json}')\n",
    "        query = input_json[\"query\"]\n",
    "        print(f'Query: {query}')\n",
    "        output = getanswer(query)\n",
    "        print(f'Output: {output}')\n",
    "        return output\n",
    "    except:\n",
    "        return jsonify({\"Status\": \"Failure --- some error occured\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "getanswer('Please give me the name of the CTO of the companies in our database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0218c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
